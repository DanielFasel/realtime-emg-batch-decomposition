{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aeed8-1d3a-46e8-8d13-33bc07affe24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install jax\n",
    "#!pip install -U \"jax[cpu]\"\n",
    "#!pip install mat73\n",
    "#!pip install scikit-learn-extra \n",
    "# Comment out the lines above after running them once. \n",
    "\n",
    "\n",
    "# Importing the libraries\n",
    "import sys\n",
    "import psutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import mat73\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59dd08-f80f-4722-b4e0-adf801a80689",
   "metadata": {},
   "source": [
    "# EMG Batch Decomposition - Calibration Algorithm\n",
    "\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836cd29-cc1f-43ba-8bc6-d59dbb85d6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This funtion extends the data. \n",
    "# It takes the data and appends a copy of it below. It does this according to the extension factor.\n",
    "# Every new copy is padded by an additional 0. First copy 1 zero, second copy 2 zeroes, etc\n",
    "\n",
    "def extend_array(array, ext_factor):\n",
    "    # Get the shape of the array.    \n",
    "    original_rows, original_columns = array.shape\n",
    "    \n",
    "    # Calculate the dimentions of the extended array\n",
    "    new_rows = original_rows * ext_factor \n",
    "    new_columns = original_columns + (ext_factor - 1)\n",
    "\n",
    "    # Initialize a new array filled with zeros\n",
    "    ext_array = np.zeros((new_rows, new_columns), dtype=array.dtype)\n",
    "\n",
    "    # Copy the original array into the new array at specific positions\n",
    "    for i in range(ext_factor):\n",
    "        ext_array[i*original_rows:(i+1)*original_rows, i:(i+original_columns)] = array # Is there a way to do this without loop? Would it be faster?\n",
    "    \n",
    "    return ext_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0fb52-2fcf-4d9f-aa41-8aea7b604b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notch filter to remove the grid 50Hz noise.\n",
    "def notch_filter(array, notch_freq, sample_freq):\n",
    "    # Create/view notch filter\n",
    "    quality_factor = 30.0  # Quality factor\n",
    "    b_notch, a_notch = signal.iirnotch(notch_freq, quality_factor, sample_freq)\n",
    "\n",
    "    # apply notch filter to signal\n",
    "    array_notched = signal.filtfilt(b_notch, a_notch, array)\n",
    "\n",
    "    return array_notched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86717b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that filters a specific frequency from the EMG signal. The EMG contains multiple channels that all nede to be filtered.\n",
    "def filter_(emg, filter_freq, sample_freq):\n",
    "    for i in range(emg.shape[0]):\n",
    "        # Loop through the channels and filter away the 50hz with a notch filter.\n",
    "        emg[i] = notch_filter(emg[i], filter_freq, sample_freq)\n",
    "    return emg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f402ff4-5aa9-40b9-91c4-05d4b46fba8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concatenate_matrices_horizontal(matrix1, matrix2):\n",
    "    # Check if the matrices have the same number of rows\n",
    "    if matrix1.shape[0] != matrix2.shape[0]:\n",
    "        raise ValueError(\"Matrices must have the same number of rows for horizontal concatenation.\")\n",
    "\n",
    "    # Concatenate matrices horizontally\n",
    "    concatenated_matrix = np.concatenate((matrix1, matrix2), axis=1)\n",
    "\n",
    "    return concatenated_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f015bf6-e117-43ba-a7c4-5fdad7c1cbf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#randomized svd by scikitlearn, truncated svd by irlb, Lanczos method svd a.o.\n",
    "#https://medium.com/@nestorarsenov/a-faster-svd-approximation-that-saves-time-and-makes-heavy-matrix-calculations-possible-310c48f5f9be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b44f0",
   "metadata": {},
   "source": [
    "## User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file path\n",
    "file_path = '../data/SIMULATION_56_40.mat'\n",
    "#file_path2 = 'data/Dof2BRep.mat'\n",
    "\n",
    "# Session Parameters\n",
    "iteration_count = 50 #NITER in the matlab code\n",
    "channel_count = 128 # Channel number\n",
    "guess_count = 300 # Count of guesses in the activity matrix\n",
    "\n",
    "# Plotting\n",
    "plotting = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e0bf6-86a7-4247-ab33-339475d4b28e",
   "metadata": {},
   "source": [
    "## Algorithm Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20e954-3b6c-48da-9d78-6d5991afaa68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables that are given by the user.\n",
    "\n",
    "# Compare with Matlab Niter = 50\n",
    "# sillouhette threshold 0.9\n",
    "# 5 sources found. Motor Unit Spike Train (Must)\n",
    "\n",
    "# Algorithm Parameters\n",
    "factor = 5; # Extention Factor = factor/number of channels. !!!! At the moment -> extention_factor = factor !!!!\n",
    "outlier_noise_interval = 5e-3 # interval to remove identified time points (in s)\n",
    "outlier_threshold = 99 # Outlier percentile\n",
    "\n",
    "# Loop Parameters (Which loop? - Better name)\n",
    "fixed_point_tolerance = np.exp(-4)\n",
    "max_iteration = 50\n",
    "\n",
    "# Peak finding Parameters\n",
    "distance_threshold = 20  # Replace with your desired distance threshold\n",
    "max_distance_ratio = 0.1  # Replace with your desired max distance ratio\n",
    "silhouette_threshold = 0.9 # Treshold below the source is not considered\n",
    "min_hz = 4 # Minimum spike train frequency for good source\n",
    "max_hz = 35 # Maximum spike train frequency for good source\n",
    "\n",
    "duplicate_threshold_value = 0.02 # Min distance between two spikes in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419bbe9-4611-489f-a452-106c42963345",
   "metadata": {},
   "source": [
    "## Data Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e47f16-d019-4c66-b4a3-a239adb77b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dictionary from the matlab data file.\n",
    "data_dict = scipy.io.loadmat(file_path)\n",
    "#data_dict = mat73.loadmat(file_path2) # Depending on the data this alternative method might be required.\n",
    "\n",
    "# Extract the sample frequency\n",
    "#sample_freq = data_dic['fsamp'][0][0]\n",
    "sample_freq = 4096\n",
    "\n",
    "# Extract the EMG data into a numpy array\n",
    "emg = np.array(data_dict['EMGN_40'])#[0:channel_count])\n",
    "#emg= concatenate_matrices_horizontal(data_dict['Dof2Rep']['bdataNeg'][0], data_dict['Dof2Rep']['bdataNeg'][1])\n",
    "#emg = emg[0:channel_count]\n",
    "\n",
    "# Delete the data_dict variable to free memory\n",
    "del data_dict\n",
    "# Extract the sample size and channel number\n",
    "sample_shape = emg.shape\n",
    "\n",
    "# Print Data Information\n",
    "print(f'The dimensions of the EMG data matrix are: {sample_shape}')\n",
    "print(f'The sample frequency is: {sample_freq}')\n",
    "\n",
    "if plotting:\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.plot(emg[0])\n",
    "    plt.title(\"Raw EMG data - First Channel\")\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a8880-f7c9-45bc-b95f-fa409d9c36e2",
   "metadata": {},
   "source": [
    "# Main Algorithm\n",
    "This algorithm is based on the paper by Franscesco Negro (Multi-channel intramuscular and surface EMG decomposition by convolutive blind source separation. J Neural Eng. 2016)\n",
    "\n",
    "It can be summarized to the following steps:\n",
    "1. Extend the Data\n",
    "2. Center by substracting the row-wise mean\n",
    "3. Whiten the data with svd\n",
    "4. Activity Extraction\n",
    "5. Making initial Guesses\n",
    "\n",
    "### Data Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323480bf-39a6-41eb-a09b-b8a2726a2add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the extention factor\n",
    "ext_factor = factor #round(factor/channel_count) #currently not used for faster iterations during developement. Needs to be changed!!\n",
    "\n",
    "# extend the data\n",
    "ext_emg = extend_array(emg, ext_factor)\n",
    "\n",
    "# the Matrix should not contain any zeroes. Therefore the padded columns are removed\n",
    "# Note that the index structure is inclusive of the first index value, but not the second index value. \n",
    "# So you provide a starting index value for the selection and an ending index value that is not included in the selection.\n",
    "ext_emg = ext_emg[:, ext_factor-1:ext_emg.shape[1]-(ext_factor-1)]\n",
    "\n",
    "# the mean is substracted to center the data row-wise before whitening\n",
    "ext_emg = ext_emg - np.mean(ext_emg, axis=1, keepdims=True)\n",
    "\n",
    "print(f'The dimension of the extended EMG data matrix are: {ext_emg.shape}')\n",
    "\n",
    "if plotting:\n",
    "    plt.matshow(ext_emg[:,300:400], fignum=100)\n",
    "    plt.gca().set_aspect('auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51fbc32-3880-4d3b-afe8-af1f432c9bbf",
   "metadata": {},
   "source": [
    "### Withening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858c8e3-6bb5-4eb0-8669-d63cb939328b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the corrolation matrix to prevent doing the SVD on the full matrix. (Is that really what is appening?)\n",
    "corr_ext_emg = np.matmul(ext_emg,  ext_emg.T) / ext_emg.shape[1] # What is this matrix really? Not quite sure I understand yet.\n",
    "\n",
    "# Compute the SVD\n",
    "U, S, V = np.linalg.svd(corr_ext_emg)   # SVD correlation matrix\n",
    "\n",
    "anti_zero = np.percentile(S, 25) # Take the lowest 20th percentile treshold as a value to prevent division by 0. (the percentile is taken as the scale of the data is not known)\n",
    "# Inverse eigenvalues. Not sure why we do this\n",
    "SI = 1 / (np.sqrt(S  + anti_zero))  \n",
    "\n",
    "# Calculate the whitening matrix\n",
    "wht_mat = (U @ np.diag(SI)) @ V # Remember that in python the SVD functio returns the transposed V. So the funciton is slightly different than in Matlab         \n",
    "\n",
    "# Whiten the extended emg data matrix.\n",
    "wht_emg = wht_mat @ ext_emg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185ed23-11b2-4172-b629-89d3da5f75b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if plotting:\n",
    "    # Check the whiteneing by comparing plots before and after. Multiplying them with itself gives a better visual representation.\n",
    "    plt.figure(1)\n",
    "    plt.matshow(ext_emg @ ext_emg.T, fignum=100)\n",
    "    plt.gca().set_aspect('auto')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.matshow(wht_emg @ wht_emg.T, fignum=100)\n",
    "    plt.gca().set_aspect('auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f1d49-1a0e-46d7-9669-2838e6d80a02",
   "metadata": {},
   "source": [
    "### Activity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activty Matrix / act_m\n",
    "# We are trying to find the maximum energy of the system for a given point in time. The assumption is that at this moment it is likely that a spike occured. Is this a good assumption?\n",
    "activity_mx = np.sum(np.abs(wht_emg) ** 2, axis=0) #First the absolute values squared, then the sum of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548d337-f09e-4844-9aac-ea339c3fbfa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the begining and end of the samples. A spike can't occure before or after the time of a full spike.\n",
    "activity_mx[0:int(outlier_noise_interval*sample_freq)+1] = 0\n",
    "activity_mx[-int(outlier_noise_interval*sample_freq)-1:] = 0\n",
    "activity_length = activity_mx.shape\n",
    "print(activity_mx.shape)\n",
    "\n",
    "# Remove outliers\n",
    "activity_outlier_treshold = np.percentile(activity_mx, outlier_threshold)    # remove some artifacts from the activity index\n",
    "index = np.where(activity_mx > activity_outlier_treshold)[0]                     # find indices where ACT > TH_ACT\n",
    "\n",
    "# Remove a band around the outliers. The outliers are possibly do to effect that lasts over time. Needs to be checked with real data. For the moment we'll use this method. Need to check with filtereing the outliers before the activity matrix and after. Not convinced that doing it after makes much sense. The spikes are so fast that the would still represent a real spike. Might just make the initial guess biased but not necessarly the spikes themselves.\n",
    "noindex = []\n",
    "for i in range(len(index)):\n",
    "    activity_mx[int(index[i]-np.round(outlier_noise_interval*sample_freq)):int(index[i]+np.round(outlier_noise_interval*sample_freq))] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a16b08-afaa-43a0-93d3-08c40a1f63da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Showing the effect of the outlier removal and the band next to them. This really nedes to be tested !!! I'm not sure the logic regarding the size of the band that is removed.\n",
    "if plotting:\n",
    "    plt.figure(4)\n",
    "    plt.plot(activity_mx[1000:2000])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddc8a8c-a3b0-4b60-bce6-bfc9eaf576da",
   "metadata": {},
   "source": [
    "### Find Initial Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae8198-a006-4a97-8ab1-44101341b80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This might need to be integreated to the main loop. If a source is strong all the initial guesses will be from that source. Which leads to unecessary calculations. Might be smarter to calculate one source and then remove all the spikes attributed to that source.\n",
    "# Find the indices of the maximum activity\n",
    "guess_indices = []\n",
    "\n",
    "# Loop to get the desired count of guesses\n",
    "for i in range(guess_count):\n",
    "    max_activity_index = np.argmax(activity_mx)# Find maximum\n",
    "    guess_indices.append(max_activity_index) # Append the result\n",
    "    \n",
    "    # remove the point and the surrounding points from the activity matrix\n",
    "    activity_mx[int(max_activity_index-np.round(outlier_noise_interval * sample_freq)):int(max_activity_index+np.round(outlier_noise_interval * sample_freq))] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c6f91-d0e9-4bb3-a35e-f19c13c7b82e",
   "metadata": {},
   "source": [
    "## Functions of Main Loop \n",
    "The code below (with the initial point finding above) is running on the main loop in the matlab code. The goal is the seperate it into processes.\n",
    "\n",
    "I will make functions fo all the different steps. This will allow me to easily test the different options to parallelize the loop. \n",
    "\n",
    "### Slicing the Whitened and Extended EMG Matrix\n",
    "The Matrix is sliced to return the column corresponding to the guessed activity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59698b2-38c9-4151-9780-0ea3aaf0e6bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Doesn't need to be in the loop\n",
    "def fetch_and_normalize_matrix_columns(matrix, column_indices, show_plot):\n",
    "    # Extract the column\n",
    "    columns = matrix[:,column_indices] \n",
    "    # Normalizing the vector\n",
    "    normalized_columns = columns/np.linalg.norm(columns, axis=0)\n",
    "    \n",
    "    # Plot the result\n",
    "    if show_plot:\n",
    "        plt.figure(5)\n",
    "        plt.plot(normalized_columns[0])\n",
    "        plt.show()\n",
    "        \n",
    "    return normalized_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1011c74-1dac-4f61-a9d1-1eebc4737728",
   "metadata": {},
   "source": [
    "### Fixed Point Algorithm\n",
    "This step is done to improve the peaks. Still not quite sure what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783aec07-fb46-44c3-8307-167127917045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_fixed_point_algorithm(fixed_point_tolerance, max_iteration, matrix, array, show_plot):\n",
    "        \n",
    "    G = lambda x: 1/2 * x**2 # Maybe more descriptive name would be better\n",
    "    DG = lambda x: x\n",
    "    # Matrix dimensions\n",
    "    mx_dimension_n = matrix.shape[0] \n",
    "    mx_dimension_m = matrix.shape[1]\n",
    "    \n",
    "    # Projecting Vectors matrix\n",
    "    pv_mx = np.zeros((mx_dimension_n, max_iteration)) # Do I need this? Why is this here?\n",
    "    # Initialize the projection vector randomly\n",
    "    initial_projection_vector = np.random.randn(mx_dimension_n, 1)\n",
    "    initial_projection_vector = initial_projection_vector / np.linalg.norm(initial_projection_vector)\n",
    "\n",
    "\n",
    "    iteration_counter = 0 \n",
    "    # Convergence condition: stop when the dot product of the current projection vector and matrix is close to 1\n",
    "    while abs(abs(np.dot(initial_projection_vector.T, array)) - 1) > fixed_point_tolerance and iteration_counter < max_iteration:\n",
    "        \n",
    "   \n",
    "        initial_projection_vector = array  # Update the projection vector for stopping at fixed_point_tolerance\n",
    "\n",
    "        # Calculate the estimation source\n",
    "        intermediate_result = np.dot(array.T, matrix)  \n",
    "   \n",
    "\n",
    "        # Update the projection vector using the fixed-point iteration\n",
    "        updated_projection_vector = np.dot(matrix, G(intermediate_result).T) / mx_dimension_m - np.sum(DG(intermediate_result)) * array / mx_dimension_m  \n",
    "        #updated_projection_vector = updated_projection_vector/np.linalg.norm(updated_projection_vector)  # Normalize the updated projection vector\n",
    "\n",
    "        # Deflation (decorrelation)\n",
    "        array = updated_projection_vector - np.dot(pv_mx, np.dot(pv_mx.T, updated_projection_vector))  \n",
    "        array = array / np.linalg.norm(array)  # Normalize the array\n",
    "\n",
    "\n",
    "        iteration_counter += 1  # Increment the loop counter\n",
    "  \n",
    "    final_projection_vector = updated_projection_vector.real   # Save the latest projecting vector\n",
    "    pv_mx[:, 1] = final_projection_vector  # For deflation 1 represents the iteration count of the entire loop!!!\n",
    "\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(6)\n",
    "        plt.title(\"final projection vector\")\n",
    "        plt.plot(final_projection_vector)\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    return pv_mx, final_projection_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b1fb1-20a4-46be-b45f-5e3eb8d9a8c2",
   "metadata": {},
   "source": [
    "### Extract the Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5307f07-cfe5-47e8-9fb0-e71a374789c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_source(projection_vector, matrix, show_plot):\n",
    "    source = projection_vector.dot(matrix)\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.figure(7)\n",
    "        plt.plot(source[0:1000])\n",
    "        plt.show()\n",
    "    \n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b26a9-2d46-44c9-82f1-552fae95bb04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data, keep_percentage=100):\n",
    "    \"\"\"\n",
    "    Remove outliers from a list based on the modified percentile values.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of numeric values\n",
    "    - keep_percentage: Percentage of samples to keep (default is 100%)\n",
    "\n",
    "    Returns:\n",
    "    - filtered_data: List with outliers removed\n",
    "    - removed_indices: Indices of the removed samples\n",
    "    \"\"\"\n",
    "    # Enumerate the data to keep track of indices\n",
    "    enumerated_data = list(enumerate(data))\n",
    "\n",
    "    # Sort the data based on values\n",
    "    sorted_data = sorted(enumerated_data, key=lambda x: x[1])\n",
    "\n",
    "    # Calculate the lower and upper percentile values based on keep_percentage\n",
    "    lower_percentile = (100 - keep_percentage) / 2\n",
    "    upper_percentile = 100 - lower_percentile\n",
    "\n",
    "    # Calculate the lower and upper bounds based on the modified percentiles\n",
    "    lower_bound = np.percentile([value[1] for value in sorted_data], lower_percentile)\n",
    "    upper_bound = np.percentile([value[1] for value in sorted_data], upper_percentile)\n",
    "\n",
    "    # Keep only the data points within the specified percentile range\n",
    "    filtered_data = [value[1] for value in sorted_data if lower_bound <= value[1] <= upper_bound]\n",
    "\n",
    "    # Get the indices of removed samples\n",
    "    removed_indices = [value[0] for value in enumerated_data if value[1] < lower_bound or value[1] > upper_bound]\n",
    "\n",
    "    return filtered_data, removed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04d852-188f-4317-91ad-b9e13a1d632b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_samples_by_indices(data, indices):\n",
    "    \"\"\"\n",
    "    Remove samples from a list based on given indices.\n",
    "\n",
    "    Parameters:\n",
    "    - data: List of values\n",
    "    - indices: List of indices to remove\n",
    "\n",
    "    Returns:\n",
    "    - List with corresponding samples removed\n",
    "    \"\"\"\n",
    "    return [value for i, value in enumerate(data) if i not in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a91880-18d3-4d3c-b93d-2dc364c284f5",
   "metadata": {},
   "source": [
    "### Find the Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf98ed5-f734-45b3-bba7-0cb38c107b77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_discharges(signal, sample_frequency, distance_threshold, max_distance_ratio, show_plot):\n",
    "    # Compute TVI (Total Variance Index)\n",
    "    total_variance_index = np.abs(signal ** 2)\n",
    "    # Normalize TVI\n",
    "    normalization_factor = np.linalg.norm(total_variance_index)\n",
    "    normalized_tvi = total_variance_index / normalization_factor\n",
    "\n",
    "    # Find peaks in the normalized TVI\n",
    "    peak_indices, _ = find_peaks(normalized_tvi, distance=distance_threshold)\n",
    "\n",
    "    # Extract peaks from TVI\n",
    "    peaks = normalized_tvi[peak_indices]\n",
    "    \n",
    "    \n",
    "    keep_percentage = 100\n",
    "    filtered_peaks, outlier_indices = remove_outliers(peaks, keep_percentage)\n",
    "    peaks = np.array(filtered_peaks)\n",
    "\n",
    "       \n",
    "    peak_indices = remove_samples_by_indices(peak_indices, outlier_indices)\n",
    "    peak_indices = np.array(peak_indices)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # Apply KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=2, init='k-means++', n_init=10)\n",
    "    # Testing if the initalisation is the problem\n",
    "    kmeans = KMeans(n_clusters=2, init='random', random_state=42, n_init=10)\n",
    "    kmeans.fit(peaks.reshape(-1, 1))\n",
    "    # Get cluster labels and centers\n",
    "    cluster_labels = kmeans.labels_\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "    # Create KMedoids model with 2 clusters\n",
    "    #kmedoids = KMedoids(n_clusters=2)\n",
    "    # Fit the model to the data\n",
    "    #kmedoids.fit(peaks.reshape(-1, 1))\n",
    "\n",
    "    # Get cluster labels and cluster centers (medoids)\n",
    "    #cluster_labels = kmedoids.labels_\n",
    "    #cluster_centers = kmedoids.cluster_centers_\n",
    "    \n",
    "    \n",
    "    # Calculate distances from cluster centers\n",
    "    #distances = kmeans.transform(peaks.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate mean cluster centeroids - This is absolutely not necessary as the cluster centers are equivalent to the means\n",
    "    mean_centeroid_1 = np.mean(normalized_tvi[peak_indices[cluster_labels == 0]])\n",
    "    mean_centeroid_2 = np.mean(normalized_tvi[peak_indices[cluster_labels == 1]])\n",
    "\n",
    "    # Determine maximum and minimum centeroids\n",
    "    maximum_centeroid, cluster_index = np.max([mean_centeroid_1, mean_centeroid_2]), np.argmax([mean_centeroid_1, mean_centeroid_2])\n",
    "\n",
    "    similarity_coefficient = maximum_centeroid\n",
    "    non_similarity_coefficient = np.min([mean_centeroid_1, mean_centeroid_2])\n",
    "\n",
    "    # Find indices of the selected cluster\n",
    "    indices_in_selected_cluster = np.where(cluster_labels == cluster_index)[0]\n",
    "\n",
    "    # Calculate Silhouette Score\n",
    "    silhouette = silhouette_score(peaks.reshape(-1, 1), cluster_labels, metric='euclidean')\n",
    "    print('Sillouhette Score')\n",
    "    print(silhouette)\n",
    "\n",
    "    # Calculate Coefficient of Variation for Inter-Spike Intervals\n",
    "    interval_indices = peak_indices[indices_in_selected_cluster]\n",
    "\n",
    "    inter_spike_intervals = np.diff(interval_indices / sample_frequency)\n",
    "    \n",
    "\n",
    "    coefficient_of_variation = np.std(inter_spike_intervals) / np.mean(inter_spike_intervals) * 100\n",
    "    \n",
    "    if show_plot:\n",
    "        # Plotting for visualization (you can customize this based on your needs)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot the original signal\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(signal, label='Original Signal')\n",
    "        plt.title('Original Signal')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot the TVI and identified peaks\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot( normalized_tvi, label='Total Variance Index')\n",
    "        plt.plot(peak_indices, peaks, 'ro', label='Peaks')\n",
    "        plt.title('Total Variance Index with Identified Peaks')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    \n",
    "    return normalized_tvi, normalization_factor, peaks, peak_indices, indices_in_selected_cluster, interval_indices, silhouette, coefficient_of_variation, cluster_centers, similarity_coefficient, non_similarity_coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83512e-6ea4-4607-8a82-c131919f88ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Remove Erronous Sources and Duplicate Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d7dff-adb1-4a65-84dd-9413a30ba5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def indices_below_threshold(lst, threshold):\n",
    "    \"\"\"\n",
    "    Returns the indices of elements in the list that are lower than the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - lst (list): The input list.\n",
    "    - threshold: The threshold value.\n",
    "\n",
    "    Returns:\n",
    "    - List of indices.\n",
    "    \"\"\"\n",
    "    return [index for index, value in enumerate(lst) if value < threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637c0f4-ee61-4757-ac23-1e2fc392acfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_peaks(timestamps, threshold):\n",
    "    \"\"\"\n",
    "    Removes peaks from the list if the time difference between adjacent peaks\n",
    "    is below the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - timestamps (list): List of chronological timestamps.\n",
    "    - threshold (int): Threshold time.\n",
    "\n",
    "    Returns:\n",
    "    - List of timestamps after removing events below the threshold.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while i < len(timestamps) - 1:\n",
    "        time_difference = timestamps[i + 1] - timestamps[i]\n",
    "\n",
    "        if time_difference < threshold:\n",
    "            # Remove the second timestamp (timestamps[i+1])\n",
    "            timestamps = np.delete(timestamps,(i + 1))\n",
    "        else:\n",
    "            # Move to the next index only if the timestamps were not removed\n",
    "            i += 1\n",
    "\n",
    "    return timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a9e440-8310-43ed-ad05-c8ef132042d4",
   "metadata": {},
   "source": [
    "# Start Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ace46e-4704-4e5a-8777-383f377c6df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_plot = False\n",
    "print(len(guess_indices))\n",
    "sliced_wht_emg = fetch_and_normalize_matrix_columns(wht_emg, guess_indices, show_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7e551-5868-4bc4-a265-1097fc75b57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Looping through the indices here\n",
    "def sequential_loop(fixed_point_tolerance, max_iteration, wht_emg, sliced_wht_emg, show_plot, sample_freq, distance_threshold, max_distance_ratio):\n",
    "    \n",
    "    # Explanation needed\n",
    "    pv_mx, final_projection_vector = apply_fixed_point_algorithm(fixed_point_tolerance, max_iteration, wht_emg, sliced_wht_emg, show_plot)\n",
    "    \n",
    "    # Explanation needed\n",
    "    source = estimate_source(final_projection_vector, wht_emg, show_plot)\n",
    "    \n",
    "    # Explanation needed\n",
    "    tvi, normalization_factor, peaks, peak_indices, selected_indices, interval_indices, silhouette, coefficient_of_variation, cluster_centers, similarity_coefficient, non_similarity_coefficient = classify_discharges(source, sample_freq, distance_threshold, max_distance_ratio, show_plot)\n",
    "    \n",
    "    # Explanation needed\n",
    "    #interval_indices = remove_duplicate_peaks(interval_indices, duplicate_threshold_value*sample_freq)\n",
    "\n",
    "    return tvi, normalization_factor, peaks, peak_indices, selected_indices, interval_indices, silhouette, coefficient_of_variation, cluster_centers, similarity_coefficient, non_similarity_coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b8c33-0770-467d-be8a-2590d1cb54cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fXcorr(Ind1, Ind2, Lim: int):\n",
    "    # Cross-correlation function\n",
    "    xcor = np.zeros(2 * Lim + 1)\n",
    "    for k in range(-Lim, Lim + 1):\n",
    "        xcor[k + Lim] = len(set(Ind1).intersection(set(np.array(Ind2) + k)))\n",
    "    return xcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96931a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463922bd-ce4f-456e-9407-ce862f85880b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#np.seterr(all='raise')\n",
    "# Looping through the indices here\n",
    "start = time.time()\n",
    "show_plot = False\n",
    "interval_indices_mx = []\n",
    "silhouette_mx = []\n",
    "coefficient_of_variation_mx = []\n",
    "recording_time = activity_length[0]/sample_freq\n",
    "tvi_mx = []\n",
    "norm_factor_mx = []\n",
    "cluster_centers_mx = []\n",
    "\n",
    "for i in range(50):\n",
    "    print(f'Source {i} is being processed.')\n",
    "    tvi, normalization_factor, peaks, peak_indices, selected_indices, interval_indices, silhouette, coefficient_of_variation, cluster_centers, similarity_coefficient, non_similarity_coefficient =  sequential_loop(fixed_point_tolerance, max_iteration, wht_emg, sliced_wht_emg[:,i], show_plot, sample_freq, distance_threshold, max_distance_ratio)\n",
    "\n",
    "    # Check if the source is good. Some of these could be cheked before.\n",
    "    if silhouette >= silhouette_threshold: # Check if the silhouette score is high enough\n",
    "        \n",
    "        if True:#(recording_time*min_hz)<len(interval_indices)<(recording_time*max_hz): # Check if htere are enough spikes in the train spike\n",
    "            interval_indices_mx.append(interval_indices)\n",
    "            silhouette_mx.append(silhouette)\n",
    "            coefficient_of_variation_mx.append(coefficient_of_variation)\n",
    "            tvi_mx.append(tvi)\n",
    "            norm_factor_mx.append(normalization_factor)\n",
    "            cluster_centers_mx.append(cluster_centers)\n",
    "    else:\n",
    "        print(silhouette)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('\\n')\n",
    "print(f'{len(interval_indices_mx)} sources found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec71bc8-76df-4414-a14b-0f44d97de440",
   "metadata": {
    "tags": []
   },
   "source": [
    "interval_indices_mx = flatten_dict(return_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9b78a-e84a-462f-8558-e1182f15d6ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "def flatten_dict(d):\n",
    "    flatten_array = []\n",
    "    for key, value in nested_dict.items():\n",
    "        for data in value:\n",
    "            flatten_array.append(data)\n",
    "    return flatten_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1cbdd5-1f84-4724-ace5-091ffd9bdc7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "nested_dict = {\n",
    "    'a': [1,2,3],\n",
    "    'h': [1,2,3]\n",
    "}\n",
    "flattened_dict = flatten_dict(nested_dict)\n",
    "print(flattened_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347ad8c-b7c8-4034-8462-9be0aaede74e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unit_count = len(interval_indices_mx)\n",
    "LW1 = 50e-3\n",
    "LW2 = 0.5e-3\n",
    "independent_unit_idx = []\n",
    "duplicate_unit_idx = []\n",
    "BUONA = []\n",
    "tvi_good = []\n",
    "silhouette_good = []\n",
    "cluster_good = []\n",
    "norm_fact_good = []\n",
    "\n",
    "for unit_idx_a in range(unit_count):\n",
    "    \n",
    "    duplicate_unit_cov = [] # Initiate the array\n",
    "    # Calculate the COV in Python\n",
    "    duplicate_unit_cov.append([unit_idx_a, coefficient_of_variation_mx[unit_idx_a]])\n",
    "  \n",
    "    if unit_idx_a not in duplicate_unit_idx: # Check if the unit a is a duplicate\n",
    "        \n",
    "        \n",
    "        for unit_idx_b in range(unit_idx_a, unit_count): \n",
    "            if unit_idx_b not in duplicate_unit_idx: # Check if the unit b is a duplicate\n",
    "            \n",
    "                \n",
    "                corr = fXcorr(interval_indices_mx[unit_idx_a], interval_indices_mx[unit_idx_b], int(np.round(LW1*sample_freq)))\n",
    "                corr_max_idx = np.argmax(corr)\n",
    "                corr_max = corr[corr_max_idx]\n",
    "\n",
    "                if corr_max_idx > np.round(LW2*sample_freq)+1 and corr_max_idx< len(corr)-np.round(LW2*sample_freq):\n",
    "                        SENS = np.sum(corr[int(corr_max_idx-np.round(LW2*sample_freq)):int(corr_max_idx+np.round(LW2*sample_freq))])#/np.max([len(interval_indices_mx[unit_idx_a]),len(interval_indices_mx{unit_idx_b})])*100\n",
    "                        \n",
    "                        if SENS>30: \n",
    "                            duplicate_unit_idx.append(unit_idx_b) \n",
    "                            duplicate_unit_cov.append([unit_idx_b, coefficient_of_variation_mx[unit_idx_b]])\n",
    "\n",
    "        index = np.argmin(duplicate_unit_cov)\n",
    "        independent_unit_idx.append(duplicate_unit_cov[index][0])\n",
    "        BUONA.append(interval_indices_mx[duplicate_unit_cov[index][0]])  \n",
    "        tvi_good.append(tvi_mx[duplicate_unit_cov[index][0]])  \n",
    "        silhouette_good.append(silhouette_mx[duplicate_unit_cov[index][0]])  \n",
    "        cluster_good.append(cluster_centers_mx[duplicate_unit_cov[index][0]])  \n",
    "        norm_fact_good.append(norm_factor_mx[duplicate_unit_cov[index][0]])  \n",
    "\n",
    "\n",
    "\n",
    "print('Result')\n",
    "print(independent_unit_idx)\n",
    "print(BUONA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42aa0e5-2441-4847-ab9a-c8349eb32bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "   if not(show_plot):\n",
    "        plt.figure(8)\n",
    "        plt.plot(tvi_mx[0][95500:98000])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2361cb-37d8-4b55-978c-ab2cf7309677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for tvi_values in tvi_good:\n",
    "    plt.figure(10)\n",
    "    plt.plot(tvi_values)\n",
    "    plt.title(f\"TVI_MX: {tvi_values[0]}\")  # You can customize the title based on your needs\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dddb800-e076-4b99-bbbb-66f6e170c010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for spikes_train in BUONA:\n",
    "    plt.figure(figsize=(40, 4))\n",
    "    time_interval_indices = spikes_train / sample_freq\n",
    "    plt.vlines(time_interval_indices, 0,1, color='black', linestyle='solid', label='Spikes')\n",
    "    # Customize the plot as needed\n",
    "    plt.xlabel('Time in S')\n",
    "    plt.title('Spike Train')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da3cf3-ff91-4d8c-bd8a-0c9049ba095f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample arrays\n",
    "array1 = [1, 2, 3,5]\n",
    "array2 = ['a', 'b', 'c',6]\n",
    "array3 = [True, False, True,5]\n",
    "\n",
    "\n",
    "# Zip the arrays together\n",
    "zipped_data = zip(BUONA, silhouette_good, cluster_good, norm_fact_good)\n",
    "\n",
    "# Create a list of dictionaries\n",
    "json_data = [{'SpikeTrain': item1.tolist(), 'SilhouetteScore': item2, 'ClusterCenters': item3.tolist(), 'NormaliationFactor': item4} for item1, item2, item3, item4 in zipped_data]\n",
    "\n",
    "# Write the list of dictionaries to a JSON file\n",
    "with open('../data/output.json', 'w') as json_file:\n",
    "    json.dump(json_data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc6d9b-68d5-475c-b57b-1861448403f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
